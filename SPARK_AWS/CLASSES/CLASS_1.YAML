1) CLASS FROM: https://www.youtube.com/watch?v=PeaLln90YXg&list=PLACD_PaYcVF0wXU-UIuC6mhvJZ0uu0TlP
    1.1) CREATE A USER IN AWS:
        - IAM -> Users -> Add user -> User name -> PROVIDE user access to AWS console 
              -> i want to create an iam user -> custom password -> set permisson 
              -> attach existing policies directly -> AdministratorAccess -> next -> create user -> download .csv -> close.
    
    1.2) CREATE A SECRET CREDENTIALS:
        - IAM -> Users -> user -> Security credentials -> create access key -> command line interface -> download .csv -> close.
          Console sign-in URL
          https://525160324146.signin.aws.amazon.com/console
          
          User name
          tonycastellamare
          
          Console password
          darakin123-
          Hide

    1.3) CREATE A BUCKET IN S3:
        - S3 -> Create bucket -> bucket name -> region -> create bucket -> close. # CREATE TWO BUCKETS: ONE FOR INPUT AND ANOTHER FOR OUTPUT.
    
    1.4) CREATE A VPC TO RUN THE EMR:
        # aws vpc is a virtual network that you define in the cloud. it closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS.
        - VPC -> Your VPCs 
              -> Create VPC 
              -> VPC name 
              -> number of availability zones 2
              -> number of public subnets 2
              -> number of private subnets 0
              -> vpc endpoint s3 gateway
              -> nat gateway  none
              -> create VPC.
        
    1.5) CREATE A CLUSTER IN EMR:
        - EMR -> Create cluster 
              -> name -> amazon emr release -> aplication bundle (spark and zeppelin, jupyterEnterprise)
              -> instance type (m5.xlarge) -> remove task -> cluster scaling and provisioning (use EMR managed scaling)
              -> minimun cluster size 3 max size 10 max core 8 max instance 8
              -> provisioning configuration (instance size 2)
              -> networking (brower to vpc created e subnet public)
              -> cluster termination (automatically terminate 1 day)
              -> cluster logs (create a new s3 bucket automaticaly)
              -> amazon ec2 key pair (create a new key pair -> .pem -> download)
              -> create a service role  -> security groups -> choose my vpc create
              -> create an instance prifile -> all s3 buckets in this account with read and write access
              -> create cluster.

    1.6) CREATE A EMR STUDIO to use jupyter notebook:
        - give a name
        - select vpc created previously
        - select both subnets
        - default security group
        - aws iam service role -> amazonemr service role 2023....
        - workspaces -> create workspace for it 
        - create studio.
        obs: if have some problemwith 'ServiceRole' -> iam -> role -> locate the number -> attach permition -> admaccess and amazon s3fullaccess
        - create a workspace 
        - launch workspace, inside notebook:
            - go to compute -> emr cluster on ec2 -> choose what i created before -> attach -> start.
                script:
                        from pyspark.sql import SparkSession
                        from pyspark.sql.functions import col
                    
                        - import https://www.redfin.com/news/data-center/ data: right button over it and select copy link address -> https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz

                            %%bash
                            # This is a Bash cell
                            wget -O - https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz | aws s3 cp - s3://lucasmatheus-test-buchet-s3/city_market_tracker.tsv000.gz

                        spark = SparkSession.builder.appName("RedfinDataAnalysis").getOrCreate()
                        redfin_data = spark.read.csv("s3://lucasmatheus-test-buchet-s3/city_market_tracker.tsv000.gz", sep="\t", header=True, inferSchema=True)
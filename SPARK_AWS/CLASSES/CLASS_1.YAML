1) CLASS FROM: https://www.youtube.com/watch?v=PeaLln90YXg&list=PLACD_PaYcVF0wXU-UIuC6mhvJZ0uu0TlP
    1.1) CREATE A USER IN AWS:
        - IAM -> Users -> Add user -> User name -> PROVIDE user access to AWS console 
              -> i want to create an iam user -> custom password -> set permisson 
              -> attach existing policies directly -> AdministratorAccess -> next -> create user -> download .csv -> close.
    
    1.2) CREATE A SECRET CREDENTIALS:
        - IAM -> Users -> user -> Security credentials -> create access key -> command line interface -> download .csv -> close.
          Console sign-in URL
          https://525160324146.signin.aws.amazon.com/console
          
          User name
          tonycastellamare
          
          Console password
          darakin123-
          Hide

    1.3) CREATE A BUCKET IN S3:
        - S3 -> Create bucket -> bucket name -> region -> create bucket -> close. # CREATE TWO BUCKETS: ONE FOR INPUT AND ANOTHER FOR OUTPUT.
    
    1.4) CREATE A VPC TO RUN THE EMR:
        # aws vpc is a virtual network that you define in the cloud. it closely resembles a traditional network that you'd operate in your own data center, with the benefits of using the scalable infrastructure of AWS.
        - VPC -> Your VPCs 
              -> Create VPC 
              -> VPC name 
              -> number of availability zones 2
              -> number of public subnets 2
              -> number of private subnets 0
              -> vpc endpoint s3 gateway
              -> nat gateway  none
              -> create VPC.
        
    1.5) CREATE A CLUSTER IN EMR:
        - EMR -> Create cluster 
              -> name -> amazon emr release -> aplication bundle (spark and zeppelin, jupyterEnterprise)
              -> instance type (m5.xlarge) -> remove task -> cluster scaling and provisioning (use EMR managed scaling)
              -> minimun cluster size 3 max size 10 max core 8 max instance 8
              -> provisioning configuration (instance size 2)
              -> networking (brower to vpc created e subnet public)
              -> cluster termination (automatically terminate 1 day)
              -> cluster logs (create a new s3 bucket automaticaly)
              -> amazon ec2 key pair (create a new key pair -> .pem -> download)
              -> create a service role  -> security groups -> choose my vpc create
              -> create an instance prifile -> all s3 buckets in this account with read and write access
              -> create cluster.

    1.6) CREATE A EMR STUDIO to use jupyter notebook:
        - give a name
        - select vpc created previously
        - select both subnets
        - default security group
        - aws iam service role -> amazonemr service role 2023....
        - workspaces -> create workspace for it 
        - create studio.
        obs: if have some problemwith 'ServiceRole' -> iam -> role -> locate the number -> attach permition -> admaccess and amazon s3fullaccess
        - create a workspace 
        - launch workspace, inside notebook:
            - go to compute -> emr cluster on ec2 -> choose what i created before -> attach -> start.
                script:
                        from pyspark.sql import SparkSession
                        from pyspark.sql.functions import col
                    
                        - import https://www.redfin.com/news/data-center/ data: right button over it and select copy link address -> https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz

                            %%bash
                            # This is a Bash cell
                            wget -O - https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/city_market_tracker.tsv000.gz | aws s3 cp - s3://lucasmatheus-test-buchet-s3/city_market_tracker.tsv000.gz

                        spark = SparkSession.builder.appName("RedfinDataAnalysis").getOrCreate()
                        redfin_data = spark.read.csv("s3://lucasmatheus-test-buchet-s3/city_market_tracker.tsv000.gz", sep="\t", header=True, inferSchema=True)
                        redfin_data.show(5)
                        redfin_data.columns
                        df_redfin = redfin_data.select("period_end", "period_duration", "city", "state", "property_type", "median_sale_price", "median_ppsf", "homes_sold", "inventory", "months_of_supply", "median_dom", "sold_above_list", "last_updated")
                        print('Number of rows in the dataset: ', df_redfin.count())

                        from pyspark.sql.functions import isnull
                        null_counts = [redfin_data.where(isnull(col_name)).count() for col_name in df_redfin.columns]
                        null_counts.show()

                        # display the results
                        for col_name, count in zip(df_redfin.columns, null_counts):
                            print(f"Column '{col_name}' has {count} null values.")

                        # check for missing values in the entire dataframe
                        remaining_count = df_redfin.na.drop().count()

                        print(f"Remaining non-null values: {remaining_count}")
                        print(f"number of missing rows:{df_redfin.count() - remaining_count}")

                        # drop missing values:
                        df_redfin = df_redfin.na.drop()
                        print(f"Remaining non-null values: {df_redfin.count()}")

                        # lets map the month number to their respective month name:
                        df_redfin = df_redfin.withColumn("period_end_month", 
                        when(col('period_end_month') == 1, 'January')
                        .when(col('period_end_month') == 2, 'February')
                        .when(col('period_end_month') == 3, 'March')
                        .when(col('period_end_month') == 4, 'April')
                        .when(col('period_end_month') == 5, 'May')
                        .when(col('period_end_month') == 6, 'June')
                        .when(col('period_end_month') == 7, 'July')
                        .when(col('period_end_month') == 8, 'August')
                        .when(col('period_end_month') == 9, 'September')
                        .when(col('period_end_month') == 10, 'October')
                        .when(col('period_end_month') == 11, 'November')
                        .when(col('period_end_month') == 12, 'December')
                        .otherwise('Unknown'))

                        # let us write the final dataframe into our s3 bucket as a parquet file:
                        s3_bucket = "s3://dataset-transform-zone-yml/redfin_data.parquet"
                        df_redfin.write.mode("overwrite").parquet(s3_bucket)
                        print("Data written to S3 bucket in parquet format.")   

                        # stop the spark session:
                        spark.stop()

    1.7) how to automate the process with Airflow step by step:
        - class: https://www.youtube.com/watch?v=hFvsMTcdysQ&list=PLACD_PaYcVF2hSjF-ayqSpQcKWPL37Qzr
        
        - creaste a instance ec2 and install dependence
        - install python3-env
        - create a virtualenv and active it 
        - create a secret access key 

        - in terninal: awd configure
                     put the access key and secret key
                     region: us-west-2
                     pip install apache-airflow
                     pip apache-airflow-providers-amazon
        - in browser: access the ip created with login
        - create a bucket in s3 again:
             one -> redfin-store-raw-data
             two -> redfin-transform-data-yml
             three -> redfin-script-yml -> for our notebook
    
    1.8) how to connect my aws airflow with my vscode:
        - download the extention 'remote - ssh'
        - in vscode: ctrl + shift + p -> remote-ssh: connect to host -> configure ssh host -> C:\Users\lucas\.ssh\config -> add the host, example:
            Host aws-airflow (name of the host)
                HostName (public ip of the host)
                User ec2-user(aws user)
                IdentityFile C:\Users\lucas\Downloads\lucasmatheus.pem (path of the .pem file)





        
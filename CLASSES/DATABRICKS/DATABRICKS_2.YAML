1) INTERN DATABRIKS COMANDS:
%fs ls /databricks-datasets/structured-streaming/events/ OR dbutils.fs.ls("/databricks-datasets/structured-streaming/events/")
%fs head /databricks-datasets/structured-streaming/events/file-1.json OR dbutils.fs.head("/databricks-datasets/structured-streaming/events/file-1.json")
%fs help OR dbutils.fs.help()
%fs cp /databricks-datasets/structured-streaming/events/file-1.json file:/tmp/file-1.json OR dbutils.fs.cp("/databricks-datasets/structured-streaming/events/file-1.json", "file:/tmp/file-1.json")
%fs mv file:/tmp/file-1.json file:/tmp/file-2.json OR dbutils.fs.mv("file:/tmp/file-1.json", "file:/tmp/file-2.json") # rename file
%fs rm file:/tmp/file-2.json OR dbutils.fs.rm("file:/tmp/file-2.json") # This command is for remove a file
%fs mkdirs file:/tmp/test OR dbutils.fs.mkdirs("file:/tmp/test")
%fs ls file:/tmp/test OR dbutils.fs.ls("file:/tmp/test")
%fs rm -r /FileStore/tables/ OR dbutils.fs.rm("/FileStore/tables/", True) # to remove a folder
# to find a file:
%fs find /FileStore/tables/ --name "*.csv" | grep "fa' # to find a file with csv extension

2) INTEGRATING DATABRICKS WITH POWER BI STEP BY STEP:
    - IN DATABRICKS:
        1) go to the cluster
        2) jdbc/odbc 
        3) get the server hostname and http path

    - IN POWER BI:
        1) get data
        2) get data from database
        3) select the source
        4) put the server hostname and http path
        5) put user and password


3) DATABRICKS DELTALAKE:
    3.1) Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads.
    3.2) it is a open source project which allows us to create a lakehouse architecture in databricks.
    3.3) it can be used with batch and streaming processing beyold get to control the transactions of data in the lakehouse.
    3.4) transactions: insert, update, delete, merge, vacuum, optimize, etc. it is all operations that we can do with delta lake.
                       it will be stored at _delta_log folder.
    3.5) we can go back to the previous version of the data by having the transaction log:
         to describe the history of the table:
            DESCRIBE HISTORY '/user/hive/warehouse/purcharses'. # to see the history of the purcharses table
         to return:
            select * from delta.`/user/hive/warehouse/purcharses` version as of 0 # to return the data from the version 0
         to clone the table:
            CREATE TABLE delta.`/user/hive/warehouse/purcharses_clone` CLONE delta.`/user/hive/warehouse/purcharses` # to clone the table
         to come back version:
            RESTORE TABLE purchases TO VERSION AS OF 1 # to come back to the version 1 or any other version
         to come back a row:
            insert into purchases select * from purchases VERSION AS 1 WHERE Id = 1 # to come back to the Id 1 of the version 1
    data source -> data lake -> delta lake -> 
    3.6) schema evolution: it is the way that we can change the schema of the table without losing the data.
         to change the schema:
            ALTER TABLE purchases ADD COLUMN (new_column STRING) # to add a new column
            ALTER TABLE purchases CHANGE COLUMN (old_column STRING COMMENT 'new comment') # to change the column
            ALTER TABLE purchases DROP COLUMN (old_column) # to drop the column